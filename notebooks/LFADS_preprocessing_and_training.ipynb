{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f38b3e29",
   "metadata": {},
   "source": [
    "#\n",
    "# Preprocessing and training of the LFADS model\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5346064-def1-4503-a5f8-50ecd5f56bed",
   "metadata": {},
   "source": [
    "This notebook demonstrates how the dataset was preprocessed for LFADS model training, using Session S6 as an example.\n",
    "\n",
    "Although the saved .pkl.gz file already includes the final LFADS and SpikeCount data, we reconstruct the intermediate steps here for reproducibility and understanding.\n",
    "\n",
    "**Note:**  \n",
    "This notebook takes as input a pre-filtered dataset saved in a gzip-compressed pickle file published as the associated dataset to the paper \"The geometry of the neural state space of decisions\" in [Zenodo](https://zenodo.org/records/15093134). The dataset originates from the raw recordings available at [Zenodo](https://zenodo.org/records/13207505), but includes only completed **decision trials** (`trialType==20`), excluding other tasks or incomplete trials.\n",
    "\n",
    "After generating the LFADS training dataset, we load the results of running AutoLFADS on NeuroCAAS and augment the DataFrame with two new columns:\n",
    "\n",
    "- `'SpikeCount'`: Trial-aligned spike count matrix used for training the LFADS model (firing rates in Hz, 10ms bins).\n",
    "- `'LFADS'`: Smoothed firing rates inferred by AutoLFADS for the same neurons and time windows.\n",
    "\n",
    "Both columns contain lists of arrays, one per trial, each of shape `(Neurons Ã— Time)`, with time cropped from dot-motion onset to saccade completion â€” so trial durations vary (not padded with zeros)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983d9df6-21a3-4065-8538-31e3f5f650f8",
   "metadata": {},
   "source": [
    "##\n",
    "## Step 0: Set root directory and download some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67196e44-44e7-4474-9f33-e8233a4c4c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Automatically find the project root (directory containing 'src' or 'data')\n",
    "def find_project_root(marker_dirs=(\"src\", \"data\",\"notebooks\")):\n",
    "    path = os.getcwd()\n",
    "    while path != \"/\" and not all(os.path.exists(os.path.join(path, d)) for d in marker_dirs):\n",
    "        path = os.path.dirname(path)\n",
    "    return path\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "\n",
    "# Set up Python import path and working directory\n",
    "sys.path.append(os.path.join(PROJECT_ROOT, \"src\"))\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8755837-f8a0-4832-8be3-3d1d6508cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from io_utils import download_session\n",
    "\n",
    "# Download and get the path to the file\n",
    "# Also sets the working session\n",
    "\n",
    "session = \"S6\"\n",
    "path = download_session(session)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc94f2",
   "metadata": {},
   "source": [
    "##\n",
    "## Step 1: Load preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ece31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.io_utils import load_dataframe_with_metadata\n",
    "\n",
    "df = load_dataframe_with_metadata(session)\n",
    "\n",
    "print(f\"Session: {df.attrs['Session']}, Monkey: {df.attrs['Monkey']}, Date: {df.attrs['Date'].date()}\")\n",
    "print(f\"Number of neurons: {df.attrs['NCells']}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a650e36",
   "metadata": {},
   "source": [
    "##\n",
    "## Step 2: Construct training and validation datasets for AutoLFADS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea09300-746d-464a-a4c5-65d145c33a95",
   "metadata": {},
   "source": [
    "We split the dataset into validation and training, mantaining a choice and coherence balanced split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f718352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "coherences = np.sort(df['coh'].unique())\n",
    "choices = np.sort(df['choice'].unique())\n",
    "train_idx, valid_idx = [], []\n",
    "\n",
    "for choice in choices:\n",
    "    for coh in coherences:\n",
    "        idx = df[(df['choice'] == choice) & (df['coh'] == coh)].index.tolist()\n",
    "        np.random.shuffle(idx)\n",
    "        split = int(2 / 3 * len(idx))\n",
    "        train_idx.extend(idx[:split])\n",
    "        valid_idx.extend(idx[split:])\n",
    "\n",
    "train_idx, valid_idx = np.array(train_idx), np.array(valid_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5916f94",
   "metadata": {},
   "source": [
    "###\n",
    "### Handling Variable Trial Durations with External Inputs in LFADS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba04fc2a-93e3-4912-8156-0aff05ed9200",
   "metadata": {},
   "source": [
    "\n",
    "We create `10ms`-binned spike count data to train the LFADS model. To mitigate artifacts from zero-padding in variable-length reaction time trials, we provide explicit external inputs to the LFADS controller. Each input is a unit step function that remains active for the duration of the trial, one per choice.\n",
    "\n",
    "This design addresses the challenges posed by the LFADS requirement for uniform trial lengths across the dataset. Without such inputs, shorter trials require significant padding, which can lead to identity overfitting and other training instabilities. By indicating the valid trial window through these step functions, we guide the model to better distinguish meaningful data from padding artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1116117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bin_size_ms = 10\n",
    "bin_size_s = bin_size_ms / 1000\n",
    "max_duration = np.max(df['saccadeComplete'] - df['dotsOn'])\n",
    "bins = np.arange(0, max_duration + bin_size_s, bin_size_s)\n",
    "times = bins[:-1] + bin_size_s / 2\n",
    "\n",
    "def process_trial(trial, df, times, bins, N):\n",
    "    t0, t1 = df['dotsOn'].loc[trial], df['saccadeComplete'].loc[trial]\n",
    "    duration = t1 - t0\n",
    "    spikes = [np.histogram(np.array(sp)-t0, bins=bins)[0] for sp in df['spCellPop'].loc[trial][:N]]\n",
    "    spike_array = np.stack(spikes, axis=-1)\n",
    "    valid = times < duration\n",
    "    left = valid & (df['choice'].loc[trial] == 0) & (times > df['dotsOn'].loc[trial]-t0) & (times < df['dotsOff'].loc[trial]-t0)\n",
    "    right = valid & (df['choice'].loc[trial] == 1) & (times > df['dotsOn'].loc[trial]-t0) & (times < df['dotsOff'].loc[trial]-t0)\n",
    "    inputs = np.stack([left, right], axis=-1).astype(float)\n",
    "    return spike_array, inputs\n",
    "\n",
    "NCells = df.attrs['NCells']\n",
    "DataT, InputsT = zip(*[process_trial(trial, df, times, bins, NCells) for trial in train_idx])\n",
    "DataV, InputsV = zip(*[process_trial(trial, df, times, bins, NCells) for trial in valid_idx])\n",
    "\n",
    "DataT, InputsT = np.stack(DataT), np.stack(InputsT)\n",
    "DataV, InputsV = np.stack(DataV), np.stack(InputsV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58714db2",
   "metadata": {},
   "source": [
    "##\n",
    "## Step 3: Save HDF5 file for AutoLFADS (NeuroCAAS compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29adff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = f\"DataLIP_10ms_{session}.h5\"\n",
    "with h5py.File(filename, 'w') as hf:\n",
    "    hf.create_dataset('train_encod_data', data=DataT)\n",
    "    hf.create_dataset('valid_encod_data', data=DataV)\n",
    "    hf.create_dataset('train_recon_data', data=DataT)\n",
    "    hf.create_dataset('valid_recon_data', data=DataV)\n",
    "    hf.create_dataset('train_ext_input', data=InputsT)\n",
    "    hf.create_dataset('valid_ext_input', data=InputsV)\n",
    "    hf.create_dataset('IndT', data=train_idx)\n",
    "    hf.create_dataset('IndV', data=valid_idx)\n",
    "\n",
    "print(f\"Saved LFADS dataset to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556f160f",
   "metadata": {},
   "source": [
    "##\n",
    "## Step 4: Running AutoLFADS on NeuroCAAS\n",
    "\n",
    "We use the preprocessed dataset `DataLIP_10ms_S6.h5` together with the config file `S6_AutoLFADS.yaml`.\n",
    "This YAML file specifies all the training parameters, architecture details, and hyperparameter search settings. Files used for all sessions are found in the `data` folder.\n",
    "\n",
    "NeuroCAAS is an online platform for reproducible cloud-based neuroscience analyses:\n",
    "Upload the HDF5 data and `S6_AutoLFADS.yaml` configuration to [NeuroCAAS](https://neurocaas.org/analysis/20) for training.\n",
    "Uploading both files to NeuroCAAS enables reproducible, scalable training of LFADS models across sessions.\n",
    "\n",
    "ðŸ“– How AutoLFADS works:\n",
    "AutoLFADS (Keshtkaran et al., 2022) is a deep learning framework based on LFADS (Pandarinath et al., 2018) that uses population-based training (PBT) to automatically tune model hyperparameters.\n",
    "It infers latent dynamical structure from neural spike trains using recurrent networks and variational inference.\n",
    "\n",
    "**References:**  \n",
    "- AutoLFADS paper: [Keshtkaran et al., 2022](https://doi.org/10.1038/s41592-022-01675-0)  \n",
    "- Original LFADS paper: [Pandarinath et al., 2018](https://doi.org/10.1038/s41592-018-0109-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8d1cf4",
   "metadata": {},
   "source": [
    "##\n",
    "## Step 5: Load LFADS results and populate DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba6a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lfads_path = f'./autolfads{session}/lfads_output_{session}.h5'\n",
    "\n",
    "with h5py.File(lfads_path, 'r') as data:\n",
    "    IndT, IndV = data['IndT'][:], data['IndV'][:]\n",
    "    lfads_train, lfads_valid = data['train_output_params'][:], data['valid_output_params'][:]\n",
    "    data_train, data_valid = data['train_encod_data'][:], data['valid_encod_data'][:]\n",
    "\n",
    "amp = 1000/bin_size_ms # to get firing rates\n",
    "df['LFADS'], df['SpikeCount'] = None, None\n",
    "\n",
    "for i, trial in enumerate(IndT):\n",
    "    t_end = int(100 * (df['saccadeDetected'].loc[trial] - df['dotsOn'].loc[trial]))\n",
    "    df.at[trial, 'LFADS'] = (amp * lfads_train[i][:t_end]).T\n",
    "    df.at[trial, 'SpikeCount'] = (amp * data_train[i][:t_end]).T\n",
    "\n",
    "for i, trial in enumerate(IndV):\n",
    "    t_end = int(100 * (df['saccadeDetected'].loc[trial] - df['dotsOn'].loc[trial]))\n",
    "    df.at[trial, 'LFADS'] = (amp * lfads_valid[i][:t_end]).T\n",
    "    df.at[trial, 'SpikeCount'] = (amp * data_valid[i][:t_end]).T\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
