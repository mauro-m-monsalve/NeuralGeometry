{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f38b3e29",
   "metadata": {},
   "source": [
    "#\n",
    "# Preprocessing and training of the LFADS model (Session S6)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5346064-def1-4503-a5f8-50ecd5f56bed",
   "metadata": {},
   "source": [
    "This notebook demonstrates how the dataset was preprocessed for LFADS model training, using Session S6 as an example.\n",
    "\n",
    "Although the saved .pkl.gz file already includes the final LFADS and SpikeCount data, we reconstruct the intermediate steps here for reproducibility and understanding.\n",
    "\n",
    "**Note:**  \n",
    "This notebook takes as input a pre-filtered dataset saved in a gzip-compressed pickle file. The dataset originates from the raw recordings available at [Zenodo](https://zenodo.org/records/13207505), but includes only completed **decision trials** (`trialType==20`), excluding other tasks or incomplete trials.\n",
    "\n",
    "After generating the LFADS training dataset, we load the results of running AutoLFADS on NeuroCAAS and augment the DataFrame with two new columns:\n",
    "\n",
    "- `'SpikeCount'`: Trial-aligned spike count matrix used for training (firing rates in Hz, 10ms bins).\n",
    "- `'LFADS'`: Smoothed firing rates inferred by AutoLFADS for the same neurons and time windows.\n",
    "\n",
    "Both columns contain lists of arrays, one per trial, each of shape `(NCells √ó TimeSteps)`, with time cropped from dots onset to saccade completion ‚Äî so trial durations vary (not padded with zeros)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc94f2",
   "metadata": {},
   "source": [
    "##\n",
    "## üß© Step 1: Load preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77ece31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.io_utils import load_dataframe_with_metadata\n",
    "\n",
    "session = \"S6\"\n",
    "df = load_dataframe_with_metadata(session)\n",
    "\n",
    "print(f\"Session: {df.attrs['Session']}, Monkey: {df.attrs['Monkey']}, Date: {df.attrs['Date'].date()}\")\n",
    "print(f\"Number of neurons: {df.attrs['NCells']}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a650e36",
   "metadata": {},
   "source": [
    "##\n",
    "## üõ†Ô∏è Step 2: Construct training and validation datasets for AutoLFADS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea09300-746d-464a-a4c5-65d145c33a95",
   "metadata": {},
   "source": [
    "We split the dataset into validation and training, mantaining a choice and coherence balanced split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f718352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "coherences = np.sort(df['coh'].unique())\n",
    "train_idx, valid_idx = [], []\n",
    "\n",
    "for choice in [0, 1]:\n",
    "    for coh in coherences:\n",
    "        idx = df[(df['choice'] == choice) & (df['coh'] == coh)].index.tolist()\n",
    "        np.random.shuffle(idx)\n",
    "        split = int(2 / 3 * len(idx))\n",
    "        train_idx.extend(idx[:split])\n",
    "        valid_idx.extend(idx[split:])\n",
    "\n",
    "train_idx, valid_idx = np.array(train_idx), np.array(valid_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5916f94",
   "metadata": {},
   "source": [
    "### Binning spikes into 10ms bins and creating inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5acdc0b-6569-4299-a33d-85ca7d0b448e",
   "metadata": {},
   "source": [
    "Inputs are just unit step functions masking the duration of the trial. There's one for each choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1116117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bin_size_ms = 10\n",
    "bin_size_s = bin_size_ms / 1000\n",
    "max_duration = np.max(df['saccadeComplete'] - df['dotsOn'])\n",
    "bins = np.arange(0, max_duration + bin_size_s, bin_size_s)\n",
    "times = bins[:-1] + bin_size_s / 2\n",
    "\n",
    "def process_trial(trial, df, times, bins, N):\n",
    "    t0, t1 = df['dotsOn'].loc[trial], df['saccadeComplete'].loc[trial]\n",
    "    duration = t1 - t0\n",
    "    spikes = [np.histogram(np.array(sp)-t0, bins=bins)[0] for sp in df['spCellPop'].loc[trial][:N]]\n",
    "    spike_array = np.stack(spikes, axis=-1)\n",
    "    valid = times < duration\n",
    "    left = valid & (df['choice'].loc[trial] == 0) & (times > df['dotsOn'].loc[trial]-t0) & (times < df['dotsOff'].loc[trial]-t0)\n",
    "    right = valid & (df['choice'].loc[trial] == 1) & (times > df['dotsOn'].loc[trial]-t0) & (times < df['dotsOff'].loc[trial]-t0)\n",
    "    inputs = np.stack([left, right], axis=-1).astype(float)\n",
    "    return spike_array, inputs\n",
    "\n",
    "NCells = df.attrs['NCells']\n",
    "DataT, InputsT = zip(*[process_trial(trial, df, times, bins, NCells) for trial in train_idx])\n",
    "DataV, InputsV = zip(*[process_trial(trial, df, times, bins, NCells) for trial in valid_idx])\n",
    "\n",
    "DataT, InputsT = np.stack(DataT), np.stack(InputsT)\n",
    "DataV, InputsV = np.stack(DataV), np.stack(InputsV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58714db2",
   "metadata": {},
   "source": [
    "##\n",
    "## üíæ Step 3: Save HDF5 file for AutoLFADS (NeuroCAAS compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29adff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = f\"DataLIP_10ms_{session}.h5\"\n",
    "with h5py.File(filename, 'w') as hf:\n",
    "    hf.create_dataset('train_encod_data', data=DataT)\n",
    "    hf.create_dataset('valid_encod_data', data=DataV)\n",
    "    hf.create_dataset('train_recon_data', data=DataT)\n",
    "    hf.create_dataset('valid_recon_data', data=DataV)\n",
    "    hf.create_dataset('train_ext_input', data=InputsT)\n",
    "    hf.create_dataset('valid_ext_input', data=InputsV)\n",
    "    hf.create_dataset('IndT', data=train_idx)\n",
    "    hf.create_dataset('IndV', data=valid_idx)\n",
    "\n",
    "print(f\"Saved LFADS dataset to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556f160f",
   "metadata": {},
   "source": [
    "##\n",
    "## üß™ Step 4: Running AutoLFADS on NeuroCAAS\n",
    "\n",
    "We use the preprocessed dataset `DataLIP_10ms_S6.h5` together with the config file `S6_AutoLFADS.yaml`.\n",
    "This YAML file specifies all the training parameters, architecture details, and hyperparameter search settings.\n",
    "\n",
    "NeuroCAAS is an online platform for reproducible cloud-based neuroscience analyses:\n",
    "Upload the HDF5 data and `S6_AutoLFADS.yaml` configuration to [NeuroCAAS](https://neurocaas.org/analysis/20) for training.\n",
    "Uploading both files to NeuroCAAS enables reproducible, scalable training of LFADS models across sessions.\n",
    "\n",
    "üìñ How AutoLFADS works:\n",
    "AutoLFADS (Keshtkaran et al., 2022) is a deep learning framework based on LFADS (Pandarinath et al., 2018) that uses population-based training (PBT) to automatically tune model hyperparameters.\n",
    "It infers latent dynamical structure from neural spike trains using recurrent networks and variational inference.\n",
    "\n",
    "**References:**  \n",
    "- AutoLFADS paper: [Keshtkaran et al., 2022](https://doi.org/10.1038/s41592-022-01675-0)  \n",
    "- Original LFADS paper: [Pandarinath et al., 2018](https://doi.org/10.1038/s41592-018-0109-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8d1cf4",
   "metadata": {},
   "source": [
    "##\n",
    "## üì• Step 5: Load LFADS results and populate DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba6a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lfads_path = f'./autolfads{session}/lfads_output_{session}.h5'\n",
    "\n",
    "with h5py.File(lfads_path, 'r') as data:\n",
    "    IndT, IndV = data['IndT'][:], data['IndV'][:]\n",
    "    lfads_train, lfads_valid = data['train_output_params'][:], data['valid_output_params'][:]\n",
    "    data_train, data_valid = data['train_encod_data'][:], data['valid_encod_data'][:]\n",
    "\n",
    "amp = 100\n",
    "df['LFADS'], df['SpikeCount'] = None, None\n",
    "\n",
    "for i, trial in enumerate(IndT):\n",
    "    t_end = int(100 * (df['saccadeDetected'].loc[trial] - df['dotsOn'].loc[trial]))\n",
    "    df.at[trial, 'LFADS'] = (amp * lfads_train[i][:t_end]).T\n",
    "    df.at[trial, 'SpikeCount'] = (amp * data_train[i][:t_end]).T\n",
    "\n",
    "for i, trial in enumerate(IndV):\n",
    "    t_end = int(100 * (df['saccadeDetected'].loc[trial] - df['dotsOn'].loc[trial]))\n",
    "    df.at[trial, 'LFADS'] = (amp * lfads_valid[i][:t_end]).T\n",
    "    df.at[trial, 'SpikeCount'] = (amp * data_valid[i][:t_end]).T\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
